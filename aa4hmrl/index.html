<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Agnostic Architecture for Heterogeneous Multi-Environment Reinforcement Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Agnostic Architecture for Heterogeneous Multi-Environment Reinforcement Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/kukjin-kim-rl/" target="_blank">Kukjin Kim</a><sup></sup>,</span>
                <span class="author-block">
                  <a href="https://rain.korea.ac.kr/members/professor" target="_blank">Changhee Joo</a><sup>*</sup></span>
                  <!-- <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                  </span> -->
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Korea University<br>NeurIPS 2023 Foundation Models for Decision Making Workshop</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openreview.net/forum?id=ypr4srFxy1" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/neverparadise/AA4HMRL" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
                </span> -->


            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser"> -->
  <!-- <div class="container is-max-desktop"> -->
    <!-- <div class="hero-body"> -->
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4" -->
        <!-- type="video/mp4"> -->
      <!-- </video> -->
      <!-- <h2 class="subtitle has-text-centered"> -->
        <!-- Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus.  -->
      <!-- </h2> -->
    <!-- </div> -->
  <!-- </div> -->
<!-- </section> -->
<!-- End teaser video -->

<section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">TL; DR</h2>
        <div class="content has-text-justified">
          <p>We propose the agnostic neural network architecture and combine distributed training algorithm for heterogeneous multi-environment reinforcement learning. This architecture is composed of multi-modal policy and multi-modal value functions. It can minimize the usage of environment-specific layers. We show that multi-environment training is possible with our architecture.</p>
        </div>
        <img src="static/images/TLDR.png" alt="MY ALT TEXT"/>
        

        <br><br>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>In new environments, training a Reinforcement Learning (RL) agent from scratch can prove to be inefficient. The computational and temporal costs can be significantly reduced if the agent can learn across diverse environments and effectively perform transfer learning. However, achieving learning across multiple environments is challenging due to the varying state and action spaces inherent in different RL problems. Padding or naive parameter-sharing with environment-specific layers for different state-action spaces are possible solutions for multi-environment training. However, these techniques are not to be scalable. In this work, we present a flexible and environment-agnostic architecture designed for learning across multiple environments simultaneously without padding or environment-specific embeddings, while enabling transfer learning for new environments. We also propose training algorithms for this architecture to enable both online and offline RL. Our experiments demonstrate that multi-environment training with one agent is possible in heterogeneous environments and parameter-sharing with environment-specific layers is not effective in transfer learning.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<section class="hero is-small">
  <div class="hero-body">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="container">
          <h2 class="title is-3">Method</h2>
          <div class="content has-text-justified">
            <h2 class="title is-5">(1) Agnostic Architecture</h2>
          </div>
          <img src="static/images/block_and_networks.png" alt="MY ALT TEXT"/>
          <h2 class="title is-6">Figure 1. Basic Building Block (Left). Policy and Value Networks (Right)</h2>
          <div class="content has-text-justified">
            <p>We want to train one multi-modal agent on heterogeneous multiple RL environments. They have each state-action space. But training in this setting requires some techniques like padding or parametrized embeddings because the input and output sizes of the neural network are fixed. To address this issue, we develop the agnostic architecture that performs sequence-to-sequence mapping. This architecture does not require padding, masking, parametrized embeddings, and a modified RL interface.</p>
          </div>
          <div class="content has-text-justified">
            <h2 class="title is-5">(2) Training Procedure</h2>
          </div>
          <img src="static/images/algorithm.png" alt="MY ALT TEXT"/>
          <h2 class="title is-6">Figure 2. Distributed Training for Multiple Environments <br> (Edited from DD-PPO, Erik Wijmans, et al.)</h2>

          <div class="content has-text-justified">
            <p>The existing gym interface does not support parallel batch processing for heterogeneous RL environments. We combine DD-PPO algorithm and our architecture for efficient parallel training. The core idea is to assign copies of the model and subset of environments on CPUs and GPUs and synchronize the gradients when training multiple environments.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>

</section>
<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="container">
          <h2 class="title is-3">Experiments</h2>
          <div class="content has-text-justified">
            <h2 class="title is-5">(1) Online Multi-Environment Training</h2>
            <!-- <h2 class="title is-6">Classic Control and Mujoco</h2> -->
          </div>
          <img src="static/images/classic_mujoco_16_env.svg" alt="MY ALT TEXT"/>
          <h2 class="title is-6">Figure 3. Results of multi-environment training in 16 heterogeneous environments <br> (Classic Control and Mujoco)</h2>
          <div class="content has-text-justified">
            <p>In the Figure 3, we observe that in most environments, the agnostic agent approaches the performance of the single env agent, and in a few environments it outperforms it. </p>
          </div>
          <!-- <div class="content has-text-justified"> -->
            <!-- <h2 class="title is-6">Mujoco and Atari</h2> -->
          <!-- </div> -->
          <br><br>
          
          <img src="static/images/atari_mujoco_16_env.png" alt="MY ALT TEXT"/>
          <h2 class="title is-6">Figure 4. Results of multi-environment training in 16 heterogeneous environments <br>  (Mujoco and Atari)</h2>
          <div class="content has-text-justified">
            <p>To train the Atari environments together, we used the Agnostic Encoder on Mujoco and the Resnet Encoder on the Atrai environments. In Figure 4, when we trained the Atari and Mujoco environments simultaneously, performance did not improve quickly on the Atari. </p>
          </div>
          <br><br>

          <div class="content has-text-justified">
            <h2 class="title is-5">(2) Offline Multi-Environment Training</h2>
          </div>
            
          <h2 class="title is-6">Table 1. Performances of DD-IQL </h2>

          <img src="static/images/DD-IQL_results.png" alt="MY ALT TEXT"/>
          <div class="content has-text-justified">
            <p>Table 1 shows the results of modifying the offline RL algorithm, IQL, on the D4RL dataset and training it with our architecture (DD-IQL). We achieve about 89% performance compared to the single-env agent. </p>
          </div>

          <br><br><br>
          <div class="content has-text-justified">
            <h2 class="title is-5">(3)  Pretraining and Transfer Learning</h2>
          </div>
          <img src="static/images/specific_vs_agnostic.png" alt="MY ALT TEXT"/>
          <div class="content has-text-justified">
            <p>In these experiments, we compare the performance of a baseline and an agnostic agent. The baseline is a setup similar to Deepmind's GATO, with environment-specific encoder and decoder layers. In transfer Learning, baseline has to add new layers and learn from scratch. </p>
          </div>

          <!-- <div class="content has-text-justified">
          <h2 class="title is-6">Pretraining</h2>
          </div> -->

        <img src="static/images/mix_to_mix_pretraining.svg" alt="MY ALT TEXT"/>
        <h2 class="title is-6">Figure 5. Pretraining</h2>

        <!-- <div class="content has-text-justified">
          <h2 class="title is-6">Transfer Learning</h2>
        </div> -->
        <img src="static/images/mix_to_mix_finetuning_scratch.svg" alt="MY ALT TEXT"/>
        <h2 class="title is-6">Figure 6. Transfer Learning</h2>

        <div class="content has-text-justified">
          <p>In the experiments, both the baseline and agnostic agents are close to the performance of agents trained on a single env. However, in transfer learning, the baseline does not learn well. We suspect that this is due to overfitting the environments trained on in pretraining. </p>
        </div>

    </div>
  </div>
</div>
</div>
</div>
</section>
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!-- BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>